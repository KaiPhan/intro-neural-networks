{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Discrete to Continous  \n",
    "\n",
    "[Watch the source Udacity video](https://www.youtube.com/watch?v=Rm2KxFaPiJg&t=62s)  \n",
    "<p align=\"center\"><img src=\"images/sigmoid1.png\" alt=\"Discrete and Continuous in Prediction\" width=\"600\">\n",
    "</p>  \n",
    "  \n",
    "In machine learning, a **discrete algorithm will classify points** as either belonging to one label or another. This is often used in classification problems where the output is a specific class or label.\n",
    "\n",
    "On the other hand, a **continuous algorithm assigns probabilities** to each point based on their distance from the decision boundary. This is often used in problems where the output is a probability that gives us a degree of certainty.  \n",
    "\n",
    "As we can see from the picture, Discrete Algorithm will tell us the point belongs which label YES or NO.  \n",
    "While the Continous Algorithm shows us the further points from the black line get more probabilities, like: 85% of being blue (for blue point in blue zone) or 20% of being blue (for red point in red zone)  \n",
    "\n",
    "The way we move from discrete predictions to continuous predictions is to simply change activattion function from Step Function to Sigmoid Function:\n",
    "\n",
    "<p align=\"center\"><img src=\"images/sigmoid2.png\" alt=\"Discrete and Continuous in Prediction\" width=\"600\">\n",
    "</p> \n",
    "\n",
    "Then the perceptron would change from Step Function to Sigmoid Function:  \n",
    "<p align=\"center\"><img src=\"images/sigmoid3.png\" alt=\"Discrete and Continuous in Prediction\" width=\"600\">\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "In a nutshell, the sigmoid function, also known as the logistic function, is a mathematical function commonly used in machine learning and neural networks.\n",
    "\n",
    "**(Take away)** Think of the sigmoid function as a magic **‘S’-shaped slide** that squashes any number into a probability **between 0 and 1**, helping machines make decisions\n",
    "\n",
    "- **Mathematical Form:** $ \\sigma(z) = \\frac{1}{1 + e^{-z}}$, where $ e$ is the base of the natural logarithm.\n",
    "\n",
    "- **Binary Classification:** In binary classification problems, the sigmoid function is often used to squash the weighted sum of inputs and biases into a probability score. If the score is above a certain threshold (commonly 0.5), the prediction is assigned to the positive class; otherwise, it belongs to the negative class.\n",
    "\n",
    "- **Activation Function:** In neural networks, the sigmoid function is employed as an activation function in the hidden layers of the network, introducing non-linearity to the model.\n",
    "\n",
    "- **Gradient Vanishing:** One limitation of the sigmoid function is the potential for gradient vanishing, particularly in deep networks. This can hinder the training process, leading to slow convergence or instability.\n",
    "\n",
    "Despite some limitations, the sigmoid function has historically played a crucial role in the development of neural networks and remains relevant, especially in scenarios where outputs need to be interpreted as probabilities or in shallow network architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
