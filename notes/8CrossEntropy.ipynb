{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "In a nutshell, there's definitely a *connection between probabilities and error functions*, and it's called **Cross-Entropy**  \n",
    "\n",
    "[Watch the source Udacity video 1](https://www.youtube.com/watch?v=iREoPUrpXvE&t=197s)  \n",
    "[Watch the source Udacity video 2](https://www.youtube.com/watch?v=1BnhC6e0TFw&t=268s)  \n",
    "\n",
    "# Binary Cross-Entropy (Log Loss)\n",
    "\n",
    "The binary cross-entropy, also known as log loss, is a common form of cross-entropy used in binary classification tasks. For a pair of true labels $y_i$ and predicted probabilities $p_i$, the binary cross-entropy is calculated as:\n",
    "\n",
    "$H(y, p) = -\\sum_{i=1}^{m} \\left(y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i)\\right)$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of instances or samples.\n",
    "- $y_i$ is the true label for instance $i$, either 0 or 1.\n",
    "- $p_i$ is the predicted probability that instance $i$ belongs to class 1.\n",
    "\n",
    "The goal during model training is often to minimize this binary cross-entropy loss, encouraging the model to make predictions that align well with the true binary labels.\n",
    "\n",
    "## Note  \n",
    "### Logarithmic Functions in Cross-Entropy\n",
    "\n",
    "In the context of logarithmic functions, \"log\" and \"ln\" refer to the same operation with different bases:\n",
    "\n",
    "- **$\\log$:** Usually, it denotes the logarithm with base 10 (common logarithm).\n",
    "- **$\\ln$:** Specifically denotes the natural logarithm with base $e$, where $e$ is Euler's number (approximately 2.71828).\n",
    "\n",
    "In the cross-entropy formula, you can use either \"log\" or \"ln\" interchangeably, as long as you are **consistent** within the formula. If you're using \"$\\ln$\" for the natural logarithm, the formula remains the same, and the result will be equivalent.\n",
    "\n",
    "So, in the binary cross-entropy formula, you can use either $\\log$ or $\\ln$, and the result will be the same:\n",
    "\n",
    "$ H(y, p) = -\\sum_{i=1}^{m} \\left(y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i)\\right) $\n",
    "\n",
    "or\n",
    "\n",
    "$ H(y, p) = -\\sum_{i=1}^{m} \\left(y_i \\cdot \\ln(p_i) + (1 - y_i) \\cdot \\ln(1 - p_i)\\right) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
