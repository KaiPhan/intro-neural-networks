{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "In a nutshell, there's definitely a *connection between probabilities and error functions*, and it's called **Cross-Entropy**  \n",
    "\n",
    "[Watch the source Udacity video 1](https://www.youtube.com/watch?v=iREoPUrpXvE&t=197s)  \n",
    "\n",
    "\n",
    "# Binary Cross-Entropy (Log Loss)  \n",
    "[Watch the source Udacity video 2](https://www.youtube.com/watch?v=1BnhC6e0TFw&t=268s)  \n",
    "\n",
    "The binary cross-entropy, also known as log loss, is a common form of cross-entropy used in binary classification tasks. For a pair of true labels $y_i$ and predicted probabilities $p_i$, the binary cross-entropy is calculated as:\n",
    "\n",
    "$H(y, p) = -\\sum_{i=1}^{m} \\left(y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i)\\right)$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of instances or samples.\n",
    "- $y_i$ is the true label for instance $i$, either 0 or 1.\n",
    "- $p_i$ is the predicted probability that instance $i$ belongs to class 1.\n",
    "\n",
    "The goal during model training is often to minimize this binary cross-entropy loss, encouraging the model to make predictions that align well with the true binary labels.\n",
    "\n",
    "# Categorical Cross-Entropy (Softmax Cross-Entropy)  \n",
    "[Watch the source Udacity video 3](https://www.youtube.com/watch?v=keDswcqkees&t=154s)  \n",
    "\n",
    "The categorical cross-entropy, also known as softmax cross-entropy, is a common form of cross-entropy used in multi-class classification tasks. For a pair of true one-hot encoded labels $y_i$ and predicted probabilities $p_i$, the categorical cross-entropy is calculated as:\n",
    "\n",
    "$ H(y, p) = -\\sum_{i=1}^{K} \\sum_{j=1}^{m} y_{ij} \\cdot \\log(p_{ij}) $\n",
    "\n",
    "where:\n",
    "- $K$ is the number of classes.\n",
    "- $m$ is the number of instances or samples.\n",
    "- $y_{ij}$ is the indicator function (1 if instance $j$ belongs to class $i$, 0 otherwise).\n",
    "- $p_{ij}$ is the predicted probability that instance $j$ belongs to class $i$.\n",
    "\n",
    "The goal during model training is often to minimize this categorical cross-entropy loss, encouraging the model to make predictions that align well with the true multi-class labels.\n",
    "\n",
    "\n",
    "## **Note 1**  \n",
    "### Logarithmic Functions in Cross-Entropy\n",
    "\n",
    "In the context of logarithmic functions, \"log\" and \"ln\" refer to the same operation with different bases:\n",
    "\n",
    "- **$\\log$:** Usually, it denotes the logarithm with base 10 (common logarithm).\n",
    "- **$\\ln$:** Specifically denotes the natural logarithm with base $e$, where $e$ is Euler's number (approximately 2.71828).\n",
    "\n",
    "In the cross-entropy formula, you can use either \"log\" or \"ln\" interchangeably, as long as you are **consistent** within the formula. If you're using \"$\\ln$\" for the natural logarithm, the formula remains the same, and the result will be equivalent.\n",
    "\n",
    "So, in the binary cross-entropy formula, you can use either $\\log$ or $\\ln$, and the result will be the same:\n",
    "\n",
    "$ H(y, p) = -\\sum_{i=1}^{m} \\left(y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i)\\right) $\n",
    "\n",
    "or\n",
    "\n",
    "$ H(y, p) = -\\sum_{i=1}^{m} \\left(y_i \\cdot \\ln(p_i) + (1 - y_i) \\cdot \\ln(1 - p_i)\\right) $\n",
    "\n",
    "## **Note 2**  \n",
    "\n",
    "The negative sum in cross-entropy loss serves two main purposes:\n",
    "\n",
    "1. **Positive Loss**: Logarithms of probabilities less than 1 are negative. The negative sum ensures the overall loss is positive, aligning with the goal of minimizing loss during model training.\n",
    "\n",
    "2. **Minimization Objective**: Optimization algorithms aim to minimize. The negative sum aligns with this, making minimizing the negative sum equivalent to maximizing the likelihood of correct classes.\n",
    "\n",
    "In short, the negative sum ensures a positive loss and aligns with the minimization objective of optimization algorithms.\n",
    "\n",
    "## **Note 3**  \n",
    "In the context of machine learning, <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">maximizing the probability</span> of the correct class (or equivalently, <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">minimizing the error function</span>) is indeed the same as <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">minimizing the cross-entropy</span>.  \n",
    "This is because cross-entropy is a measure of the difference between two probability distributions, and in this case, we want our predicted distribution (the output of our model) to be as close as possible to the true distribution (the actual labels). So, by <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">minimizing the cross-entropy</span>, we are effectively <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">maximizing the probability</span> of the correct class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
