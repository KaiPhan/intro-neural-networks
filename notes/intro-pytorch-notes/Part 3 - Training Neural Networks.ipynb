{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d10e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "model2 = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "## what are difference between 2 models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebd6c0",
   "metadata": {},
   "source": [
    "The main difference between the two models is that model1 has a nn.LogSoftmax(dim=1) layer at the end, while model2 does not. \n",
    "This means that model1 applies the logarithm of the softmax function to the output of the last linear layer, while model2 just outputs the **raw scores** from the last linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9271e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0cdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2acb4e",
   "metadata": {},
   "source": [
    "`optimizer.zero_grad()`. When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1)) ## output layer\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Training pass\n",
    "        \n",
    "        ##clears the accumulated gradient from the previous iteration\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        output = model(images)\n",
    "        \n",
    "        ##computes the loss value for the current batch\n",
    "        loss = criterion(output, labels) \n",
    "        \n",
    "        ##calculates the gradient of the loss with respect to the model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        ##updates the model parameters using the gradient and the learning rate\n",
    "        optimizer.step()\n",
    "        \n",
    "        ##Accumulate loss over one epoch\n",
    "        running_loss += loss.item() \n",
    "        \n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
