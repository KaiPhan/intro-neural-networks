{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776070a0",
   "metadata": {},
   "source": [
    "### PyTorch Sum with `dim` Parameter\n",
    "\n",
    "In PyTorch, the `torch.sum` function is used to calculate the sum of elements in a tensor. By using the `dim` parameter, you can specify the dimension along which you want to perform the sum.\n",
    "\n",
    "1. **`dim=0` - Sum along Rows (summing elements along columns):**\n",
    "   - If you set `dim=0`, PyTorch will perform the sum vertically, meaning across each column.\n",
    "   - **Example with a 3x3 matrix:**\n",
    "     ```python\n",
    "     import torch\n",
    "\n",
    "     matrix = torch.tensor([[1, 2, 3],\n",
    "                            [4, 5, 6],\n",
    "                            [7, 8, 9]])\n",
    "\n",
    "     sum_along_columns = torch.sum(matrix, dim=0)\n",
    "     print(sum_along_columns)\n",
    "     ```\n",
    "     The result will be `tensor([12, 15, 18])`, where each element of the resulting tensor is the sum of the corresponding column.\n",
    "\n",
    "2. **`dim=1` - Sum along Columns (summing elements along rows):**\n",
    "   - If you set `dim=1`, PyTorch will perform the sum horizontally, meaning across each row.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     import torch\n",
    "\n",
    "     matrix = torch.tensor([[1, 2, 3],\n",
    "                            [4, 5, 6],\n",
    "                            [7, 8, 9]])\n",
    "\n",
    "     sum_along_rows = torch.sum(matrix, dim=1)\n",
    "     print(sum_along_rows)\n",
    "     ```\n",
    "     The result will be `tensor([ 6, 15, 24])`, where each element of the resulting tensor is the sum of the corresponding row.\n",
    "\n",
    "This allows you to choose how you want to calculate the sum along different directions in your tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6d7ce",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c886202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19667fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79158ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2135f",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "| Activation Function | Purpose                                      | Output Range                 | Common Usage                                   |\n",
    "|---------------------|----------------------------------------------|------------------------------|------------------------------------------------|\n",
    "| Softmax             | Multi-class classification probabilities    | [0, 1] (probability values) | Output layer in multi-class classification       |\n",
    "| tanh                | Introducing non-linearity, zero-centered     | [-1, 1]                      | Hidden layers, mitigating vanishing gradient     |\n",
    "| ReLU                | Introducing non-linearity, unbounded         | [0, âˆž)                       | Hidden layers, widely used for simplicity       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b36d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
