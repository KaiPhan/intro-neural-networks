{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Function  \n",
    "  \n",
    "The softmax function, which is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.  \n",
    "\n",
    "- **Mathematical Form:** The softmax function is defined as follows for an input vector \\( z \\):\n",
    "\n",
    "  $ \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $\n",
    "\n",
    "  where $K$ is the number of classes, and $z_i$ is the raw score of class $i$ in the input vector.  \n",
    "\n",
    "  <p align=\"center\"><img src=\"images/softmax1.png\" alt=\"Muulti-class Problem\" width=\"600\">\n",
    "</p>  \n",
    "\n",
    "## Why use $exp$?\n",
    "Ans: $exp$ function turns every number into a positive number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Function (aka Soft version of max)\n",
    "\n",
    "In a nutshell, the softmax function is a mathematical function commonly used in machine learning, particularly in multi-class classification problems. Here are the key points about the softmax function:\n",
    "\n",
    "- **Purpose:** The softmax function is employed to convert a vector of raw scores (also known as logits) into a probability distribution. This distribution assigns probabilities to different classes, ensuring that they sum up to 1.\n",
    "\n",
    "- **Output Range:** The output of the softmax function for each element in the input vector is a value between 0 and 1. These values represent the estimated probabilities of each class.\n",
    "\n",
    "- **Normalization:** The exponentiation in the softmax function ensures that the values are non-negative, and the division by the sum ensures normalization, making the output a valid probability distribution.\n",
    "\n",
    "- **Common Usage:** Softmax is often used in the output layer of neural networks for multi-class classification problems. It helps in translating raw scores into meaningful class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result\n",
    "    \n",
    "    # Note: The function np.divide can also be used here, as follows:\n",
    "    # def softmax(L):\n",
    "    #     expL = np.exp(L)\n",
    "    #     return np.divide (expL, expL.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
