{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Algorithm\n",
    "\n",
    "[Watch the source Udacity video](https://www.youtube.com/watch?v=snxmBgi_GeU&t=3s)\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Initialize weights ($w_1, \\ldots, w_n$) and bias ($b$) to small random or zero values.\n",
    "\n",
    "2. **Iterative Optimization:**\n",
    "   - For each training example $(x_1, \\ldots, x_n)$:\n",
    "     - Calculate the predicted output: $ \\hat{y} = \\sigma(w_1x_1 + \\ldots + w_nx_n + b) $\n",
    "     - Update weights and bias using stochastic gradient descent:\n",
    "       $ w'_i \\leftarrow  w_i - \\frac{\\alpha}{m} (\\hat{y} - y)x_i $\n",
    "       $ b' \\leftarrow  b - \\frac{\\alpha}{m} (\\hat{y} - y) $\n",
    "     - Here, $ \\alpha $ is the learning rate, and $ m $ is the number of training examples.\n",
    "\n",
    "3. **Loss Calculation:**\n",
    "   - Use a loss function (e.g., logistic loss) to quantify the difference between predictions and actual labels.\n",
    "\n",
    "4. **Repeat:**\n",
    "   - Repeat the optimization process until convergence or a predefined number of iterations.\n",
    "\n",
    "5. **Minimize Error:**\n",
    "   - The algorithm aims to minimize the error by adjusting weights and bias, obtaining a better model for binary classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Feature**                   | **Perceptron**                                                           | **Logistic Regression**                                          |\n",
    "|:-----------------------------:|:-----------------------------------------------------------------------:|:----------------------------------------------------------------:|\n",
    "| **Output Values**             | Produces *binary outputs* (0 or 1).                                      | Produces *continuous outputs* between 0 and 1, representing probabilities.|\n",
    "| **Activation Function**       | Uses a *step function*. Output determined by a threshold.                | Uses the logistic (*sigmoid*) function, providing smooth outputs between 0 and 1.|\n",
    "| **Loss Function**             | Minimizes misclassifications. Predicted output matches the true output (0 or 1).| Minimizes logistic loss (cross-entropy). Considers entire probability distribution.|\n",
    "| **Learning Rule**             | Adjusts weights based on misclassifications. Updates weights in the direction of misclassification.| Adjusts weights using gradient descent to minimize logistic loss. Considers probabilistic nature of predictions.|\n",
    "| **Convergence**               | Guaranteed to converge if data is linearly separable.                   | Converges even if data is not linearly separable. Aims to find optimal decision boundary.|\n",
    "| **Probabilistic Interpretation** | Lacks probabilistic interpretation. Provides binary classification. | Provides probabilistic interpretation. Output represents probability of belonging to a class.|\n",
    "| **Use Cases**                 | Simple, historically relevant, used for linearly separable problems.    | Widely used in various applications. Suitable for both linear and non-linear problems.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
