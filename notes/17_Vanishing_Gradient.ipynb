{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient  \n",
    "\n",
    "When calculating the derivative of the sigmoid function at points that are very far to the right or left, the result often approaches 0. This behavior can be explained by examining the sigmoid function itself.\n",
    "\n",
    "The sigmoid function, denoted as $ \\sigma(x) $, is defined as:\n",
    "\n",
    "$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "\n",
    "As $x$ becomes very large (positively or negatively), the term $e^{-x}$ dominates the denominator. When $x$ is very large, $e^{-x}$ becomes extremely close to zero, resulting in:\n",
    "\n",
    "$ \\sigma(x) \\approx \\frac{1}{1 + 0} $\n",
    "\n",
    "This simplifies to $\\sigma(x) \\approx 1 $.\n",
    "\n",
    "Now, when you calculate the derivative of the sigmoid function, denoted as $ \\sigma'(x) $, with respect to $x$, using the quotient rule, you get:\n",
    "\n",
    "$ \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) $\n",
    "\n",
    "As $ \\sigma(x)$ approaches 1 (for very large positive $x$), the entire expression $ \\sigma(x) \\cdot (1 - \\sigma(x)) $ approaches 0. Similarly, for very large negative $x$, $ \\sigma(x) $ approaches 0, making $ \\sigma(x) \\cdot (1 - \\sigma(x)) $ also approach 0.\n",
    "\n",
    "## Vanishing Gradient Problem  \n",
    "\n",
    "The vanishing gradient problem refers to a situation in neural networks where the gradients of the loss function become extremely small as they are backpropagated to the earlier layers during training. This occurs particularly in deep networks with many layers, especially when using activation functions that squash their input, such as the sigmoid or hyperbolic tangent (tanh) functions.\n",
    "\n",
    "In essence, as the gradients diminish when moving backward through the network during training, the weights in the early layers receive very small updates. Consequently, these early layers may learn very slowly or not at all, hindering the overall learning process and limiting the capacity of the network to capture complex patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem can be mitigated by changing the activation function used in neural networks. Two commonly employed activation functions are the Hyperbolic Tangent (tanh) function and the Rectified Linear Unit (ReLU).\n",
    "\n",
    "## Hyperbolic Tangent Function\n",
    "\n",
    "This is similar to the Sigmoid Function but has a range from -1 to 1, which can handle larger values of x better. The Hyperbolic Tangent Function, denoted as $ \\tanh(x) $, is defined as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The derivative of the Hyperbolic Tangent Function, denoted as $ \\tanh'(x) $, is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tanh'(x) = 1 - \\tanh^2(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Unlike the Sigmoid Function, whose derivative approaches 0 for large positive or negative values of $x$, the Hyperbolic Tangent Functionâ€™s derivative is always between 0 and 1, and does not saturate as quickly. This means that the gradients of the Loss Function will not vanish as fast when using the Hyperbolic Tangent Function as the Activation Function, and the Weights in the Early Layers will receive larger updates during training.\n",
    "\n",
    "\n",
    "[Explore more activation functions here](https://www.youtube.com/watch?v=kA-1vUt6cvQ)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"text-align:center;\"><img src=\"images/tanh_func.png\" width=\"800\" height=\"400\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "image_path = \"images/tanh_func.png\"\n",
    "HTML(f'<div style=\"text-align:center;\"><img src=\"{image_path}\" width=\"{800}\" height=\"{400}\"></div>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "ReLU stands for Rectified Linear Unit, and it is one of the most popular Activation Functions in deep learning. ReLU is a simple function that returns the same value if $x$ is positive, but returns 0 if $x$ is negative. The ReLU function, denoted as $ \\text{ReLU}(x) $, is defined as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The derivative of the ReLU function, denoted as $ \\text{ReLU}'(x) $, is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{ReLU}'(x) = \\begin{cases} 1, & \\text{if } x > 0 \\\\ 0, & \\text{if } x \\leq 0 \\end{cases} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The ReLU function has several advantages over the Sigmoid and Hyperbolic Tangent Functions:\n",
    "\n",
    "- It is computationally efficient, as it does not involve any exponential operations.\n",
    "- It is sparse, outputting 0 for negative inputs, which can reduce the number of active neurons and prevent overfitting.\n",
    "- It does not suffer from the Vanishing Gradient Problem, as its derivative is either 0 or 1, independent of the value of $x$. This ensures consistent updates for the weights in the early layers during training.\n",
    "\n",
    "However, ReLU also has some drawbacks, such as the possibility of dying neurons (when the weights become too negative, and the ReLU outputs 0 for all inputs), and the lack of symmetry around the origin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"text-align:center;\"><img src=\"images/relu.png\" width=\"800\" height=\"400\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"images/relu.png\"\n",
    "HTML(f'<div style=\"text-align:center;\"><img src=\"{image_path}\" width=\"{800}\" height=\"{400}\"></div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">tanh and ReLU</span> can handle large x better than the sigmoid activation function because <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">their derivatives do not approach 0 when x increases or decreases </span>. This means that these functions do not saturate when x has extreme values, and therefore do not cause the vanishing gradient problem in the training process of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function does not directly update the weights, but it affects the gradient descent algorithm indirectly. \n",
    "\n",
    "The gradient descent algorithm updates the weights by calculating the partial derivatives of the loss function with respect to each weight. The loss function depends on the output of the neural network, which is computed by applying the activation function to the weighted sum of the inputs. Therefore, the partial derivatives of the loss function also depend on the activation function and its derivative.\n",
    "\n",
    "The activation function and its derivative determine how much the output of a node changes with respect to its input. If the activation function is very flat or has a very small derivative, then the output of the node will not change much even if the input changes a lot. This means that the loss function will also not change much with respect to the weight, and the partial derivative will be very small. This leads to the vanishing gradient problem, where the gradients become very small and the weights are updated very slowly.  \n",
    "\n",
    "On the other hand, if the activation function is steep or has a large derivative, then the output of the node will change a lot even if the input changes a little. This means that the loss function will also change a lot with respect to the weight, and the partial derivative will be large. This can lead to the exploding gradient problem, where the gradients become very large and the weights are updated too much.\n",
    "\n",
    "Therefore, the activation function and its derivative affect the gradient descent algorithm by influencing the magnitude of the gradients and the speed of the weight updates. The choice of the activation function is important for the performance and convergence of the neural network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
