{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Optimization\n",
    "\n",
    "## 1. Why Training Optimization Matters\n",
    "\n",
    "Optimizing the training process is crucial in overcoming challenges that can impact machine learning model performance. Issues like noisy data, suboptimal architecture, convergence problems, and computational efficiency can be effectively addressed through optimization techniques. These methods ensure the model adapts effectively to the data, converges to optimal parameter values, and performs efficiently. In essence, training optimization is pivotal for enhancing and fine-tuning machine learning models.\n",
    "\n",
    "## 2. Testing\n",
    "\n",
    "Testing is a critical step in evaluating the performance and accuracy of different models. This involves splitting the dataset into two parts: a training set and a test set. The objective is to assess how well a trained model performs on data it hasn't encountered during training.\n",
    "\n",
    "## 3. Overfitting and Underfitting\n",
    "\n",
    "[Watch Udacity video](https://www.youtube.com/watch?v=xj4PlXMsN-Y&t=25s)\n",
    "\n",
    "**Overfitting Explanation:**\n",
    "Overfitting occurs when a model excessively fits the training data, memorizing specific details or noise. This narrow focus hinders the model's ability to generalize to new and unseen data, resulting in poor performance.\n",
    "\n",
    "**Underfitting Explanation:**\n",
    "Underfitting arises when a model fails to capture enough complexity from the training data, leading to a simplistic representation. This simplistic approach struggles to adapt to underlying patterns and variability in both the training and test data, resulting in inaccurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"text-align:center;\"><img src=\"images/overfit1.png\" width=\"600\" height=\"300\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Specify the file path\n",
    "image_path = \"images/overfit1.png\"\n",
    "\n",
    "# Set the desired width and height\n",
    "width = 600  # in pixels\n",
    "height = 300  # in pixels\n",
    "\n",
    "# Use HTML to display the image with the specified size and center alignment\n",
    "HTML(f'<div style=\"text-align:center;\"><img src=\"{image_path}\" width=\"{width}\" height=\"{height}\"></div>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training Techniques\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "[Watch Early Stopping video](https://www.youtube.com/watch?v=NnS0FJyVcDQ&t=216s)\n",
    "\n",
    "**Description:** Technique to stop the training process before it completes all epochs based on a predefined criterion.  \n",
    "**Purpose:** Prevents overfitting by monitoring the model's performance on a validation set and stopping when it starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"text-align:center;\"><img src=\"images/earlystop1.png\" width=\"600\" height=\"300\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"images/earlystop1.png\"\n",
    "HTML(f'<div style=\"text-align:center;\"><img src=\"{image_path}\" width=\"{width}\" height=\"{height}\"></div>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization  \n",
    "[Watch Udacity Regularization video](https://www.youtube.com/watch?v=ndYnUrx8xvs&t=343s)\n",
    "\n",
    "**Description:** Set of techniques to prevent overfitting and improve the model's ability to generalize to new data.  \n",
    "**Purpose:** Introduces constraints or penalties on the model parameters to avoid overly complex models that fit the training data too closely.\n",
    "\n",
    "### Takeaway:\n",
    "- Large weights can lead to overfitting and slow convergence.\n",
    "- Regularization constrains weights to avoid overfitting, promoting a balanced model.\n",
    "\n",
    "### Mechanism of Regularization:\n",
    "Regularization involves adding a control term to the loss function to minimize the weights of the model. Two common types are:\n",
    "\n",
    "### L1 Regularization (LASSO):\n",
    "- **Mechanism:** Adds a term to the loss function proportional to the absolute values of the weights.\n",
    "- **Effect:** Creates pressure for some weights to become precisely zero, leading to sparsity in the weights. Good for feature selection.\n",
    "\n",
    "### L2 Regularization (Ridge):\n",
    "- **Mechanism:** Adds a term to the loss function proportional to the squared values of the weights.\n",
    "- **Effect:** Induces weights to have smaller values, preventing a few weights from becoming excessively large compared to others. Normally better for training models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"text-align:center;\"><img src=\"images/regularization.png\" width=\"800\" height=\"300\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"images/regularization.png\"\n",
    "HTML(f'<div style=\"text-align:center;\"><img src=\"{image_path}\" width=\"{800}\" height=\"{height}\"></div>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dropout**  \n",
    "[Watch Udacity Dropout video](https://www.youtube.com/watch?v=Ty6K6YiGdBs&t=129s)\n",
    "\n",
    "Dropout is a regularization technique used in neural networks during training. It involves randomly \"dropping out\" (i.e., setting to zero) a subset of neurons or units in the neural network during each iteration of training. This means that for each update of the model's weights, a different random subset of neurons is ignored.\n",
    "\n",
    "**Purpose:**\n",
    "- **Promoting Robustness:** Dropout helps prevent the co-adaptation of neurons, forcing the network to rely on a more diverse set of features and preventing overreliance on specific neurons.\n",
    "  \n",
    "**Key Points:**\n",
    "- **Random Deactivation:** Neurons are randomly deactivated during each training iteration.\n",
    "- **Training vs. Testing:** Dropout is typically applied during training, and all neurons are used during testing (no dropout) to make predictions.\n",
    "- **Regularization:** Acts as a form of regularization, improving the model's generalization to new, unseen data.\n",
    "\n",
    "**Takeaway:**\n",
    "Dropout is like randomly \"switching off\" some neurons during training, forcing the network to be more resilient, adaptive, and less likely to overfit to the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
