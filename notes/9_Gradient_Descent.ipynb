{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Simplified\n",
    "\n",
    "## Gradient Calculation  \n",
    "- The sigmoid function's derivative is $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$. \n",
    "\n",
    "- The error formula is $E = -\\frac{1}{m} \\sum_{i=1}^{m} \\left(y_i \\ln(\\hat{y}_i) + (1 - y_i)\\ln(1 - \\hat{y}_i)\\right)$, where $\\hat{y_i} = \\sigma(Wx^{(i)} + b)$.\n",
    "\n",
    "- The gradient of $E$ at a point $\\mathbf{x} = (x_1, \\ldots, x_n)$ is $\\nabla E = \\left(\\frac{\\partial}{\\partial w_1} E, \\ldots, \\frac{\\partial}{\\partial w_n} E, \\frac{\\partial}{\\partial b} E\\right)$.\n",
    "\n",
    "- The error produced by each point is $E = -y \\ln(\\hat{y}) - (1 - y) \\ln(1 - \\hat{y})$.\n",
    "\n",
    "- The derivative of this error with respect to the weights is $\\frac{\\partial}{\\partial w_j} \\hat{y} = \\hat{y}(1 - \\hat{y}) x_j$.\n",
    "\n",
    "- The derivative of the error $E$ at a point $\\mathbf{y}, \\mathbf{x}$, with respect to the weight $w_j$ is $\\frac{\\partial}{\\partial w_j} E = -(y - \\hat{y}) x_j$.\n",
    "\n",
    "- A similar calculation shows that $\\frac{\\partial E}{\\partial b} = -(y - \\hat{y})$.\n",
    "\n",
    "In summary, the gradient is $\\nabla E = - (y - \\hat{y}) (x_1, \\ldots, x_n, 1)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Proof\n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Sigmoid Derivative</span>\n",
    "\n",
    "The derivative of the sigmoid function is given by $ \\sigma'(x) = \\sigma(x) (1 - \\sigma(x)) $. To derive this, we utilize the quotient rule:\n",
    "\n",
    "$ \\sigma'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\sigma(x) \\cdot (1 - \\sigma(x)) $\n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Error Function and Gradient</span>\n",
    "\n",
    "Consider the error function for a set of points with labels and predictions:\n",
    "\n",
    "$ E = -\\frac{1}{m} \\sum_{i=1}^{m} \\left(y_i \\ln(\\hat{y}_i) + (1 - y_i)\\ln(1 - \\hat{y}_i)\\right) $\n",
    "\n",
    "Where the prediction is given by $hat{y_i} = \\sigma(Wx^{(i)} + b) $.\n",
    "\n",
    "The goal is to calculate the gradient of $$ at a point $ \\mathbf{x} = (x_1, \\ldots, x_n) $, given by:\n",
    "\n",
    "$ \\nabla E = \\left(\\frac{\\partial}{\\partial w_1} E, \\ldots, \\frac{\\partial}{\\partial w_n} E, \\frac{\\partial}{\\partial b} E\\right) $\n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Derivative of Error with Respect to Weights</span>\n",
    "\n",
    "The derivative of the error with respect to weights is found by calculating $ \\frac{\\partial}{\\partial w_j} \\hat{y} $, where $ \\hat{y} = \\sigma(Wx + b) $:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j} \\hat{y} = \\hat{y}(1 - \\hat{y}) x_j $\n",
    "\n",
    "Now, let's proceed with the proof.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial w_j} \\hat{y} &= \\frac{\\partial \\sigma}{\\partial w_j} (Wx + b) \\\\\n",
    "&= \\sigma(Wx + b)(1 - \\sigma(Wx + b)) \\frac{\\partial}{\\partial w_j} (Wx + b) \\\\  \n",
    "&= \\hat{y}(1 - \\hat{y}) \\frac{\\partial}{\\partial w_j} (Wx + b)\\\\\n",
    "&= \\hat{y}(1 - \\hat{y}) \\frac{\\partial }{\\partial w_j} (w_1x_1  + \\ldots + w_jx_j + \\ldots + w_nx_n + b) \\\\  \n",
    "&= \\hat{y}(1 - \\hat{y}) x_j \\\\\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Derivative of Error</span>\n",
    "\n",
    "The derivative of the error at a point $ \\mathbf{y}, \\mathbf{x} $, with respect to weight $ w_j $:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j} E = -(y - \\hat{y}) x_j $\n",
    "\n",
    "Now, we can go ahead and calculate the derivative of the error $E$ at a point $\\mathbf{y}, \\mathbf{x}$, with respect to the weight $w_j$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial w_j} E &= \\frac{\\partial}{\\partial w_j}[-y\\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})] \\\\\n",
    "&= -y \\frac{\\partial}{\\partial w_j} \\log(\\hat{y}) - (1 - y) \\frac{\\partial}{\\partial w_j} \\log(1 - \\hat{y}) \\\\\n",
    "&= -y \\frac{1}{\\hat{y}} \\frac{\\partial}{\\partial w_j}\\hat{y} - (1 - y) \\frac{1}{1 - \\hat{y}} \\frac{\\partial}{\\partial w_j}( 1- \\hat{y}) \\\\\n",
    "&= -y(1 - \\hat{y}) x_j + (1 - y)\\hat{y} x_j \\\\\n",
    "&= -(y - \\hat{y}) x_j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "A similar calculation shows that:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial b} = -(y - \\hat{y}) $\n",
    "\n",
    "In summary, the gradient is expressed as:\n",
    "\n",
    "$ \\nabla E = - (y - \\hat{y}) (x_1, \\ldots, x_n, 1) $\n",
    "\n",
    "This reveals that the gradient is a scalar multiple of the coordinates of the point, with the scalar being the difference between the label and the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz  \n",
    "What does the scalar we obtained above signify? (Check all that are true.)  \n",
    "- [] Closer the label to the prediction, larger the gradient.  \n",
    "- [x] Closer the label to the prediction, smaller the gradient.  \n",
    "- [x] Farther the label from the prediction, larger the gradient.  \n",
    "- [] Farther the label to the prediction, smaller the gradient.  \n",
    "\n",
    "So, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Step\n",
    "\n",
    "The gradient descent step involves updating the weights and bias based on the negative gradient of the error function at each point. The updates are performed as follows:\n",
    "\n",
    "For weights:\n",
    "$ w_i' \\leftarrow w_i - \\alpha \\cdot [-(y - \\hat{y}) \\cdot x_i] $\n",
    "\n",
    "This can be expressed as:\n",
    "$ w_i' \\leftarrow w_i + \\alpha \\cdot (y - \\hat{y}) \\cdot x_i $\n",
    "\n",
    "For the bias:\n",
    "$ b' \\leftarrow b + \\alpha \\cdot (y - \\hat{y}) $\n",
    "\n",
    "Here, $ \\alpha $ is the learning rate, and by considering the average of errors, we can represent it as $ \\frac{1}{m} \\cdot \\alpha $, but for simplicity, we denote it as $ \\alpha $.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
