{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Simplified\n",
    "\n",
    "## Gradient Calculation  \n",
    "- The sigmoid function's derivative is $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$. \n",
    "\n",
    "- The error formula is $E = -\\frac{1}{m} \\sum_{i=1}^{m} \\left(y_i \\ln(\\hat{y}_i) + (1 - y_i)\\ln(1 - \\hat{y}_i)\\right)$, where $\\hat{y_i} = \\sigma(Wx^{(i)} + b)$.\n",
    "\n",
    "- The gradient of $E$ at a point $\\mathbf{x} = (x_1, \\ldots, x_n)$ is $\\nabla E = \\left(\\frac{\\partial}{\\partial w_1} E, \\ldots, \\frac{\\partial}{\\partial w_n} E, \\frac{\\partial}{\\partial b} E\\right)$.\n",
    "\n",
    "- The error produced by each point is $E = -y \\ln(\\hat{y}) - (1 - y) \\ln(1 - \\hat{y})$.\n",
    "\n",
    "- The derivative of this error with respect to the weights is $\\frac{\\partial}{\\partial w_j} \\hat{y} = \\hat{y}(1 - \\hat{y}) x_j$.\n",
    "\n",
    "- The derivative of the error $E$ at a point $\\mathbf{y}, \\mathbf{x}$, with respect to the weight $w_j$ is $\\frac{\\partial}{\\partial w_j} E = -(y - \\hat{y}) x_j$.\n",
    "\n",
    "- A similar calculation shows that $\\frac{\\partial E}{\\partial b} = -(y - \\hat{y})$.\n",
    "\n",
    "In summary, the gradient is $\\nabla E = - (y - \\hat{y}) (x_1, \\ldots, x_n, 1)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Proof\n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Sigmoid Derivative</span>\n",
    "\n",
    "The derivative of the sigmoid function is given by $ \\sigma'(x) = \\sigma(x) (1 - \\sigma(x)) $. To derive this, we utilize the quotient rule:\n",
    "\n",
    "$ \\sigma'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\sigma(x) \\cdot (1 - \\sigma(x)) $\n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Error Function and Gradient</span>\n",
    "\n",
    "Consider the error function for a set of points with labels and predictions:\n",
    "\n",
    "$ E = -\\frac{1}{m} \\sum_{i=1}^{m} \\left(y_i \\ln(\\hat{y}_i) + (1 - y_i)\\ln(1 - \\hat{y}_i)\\right) $\n",
    "\n",
    "Where the prediction is given by $\\hat{y_i} = \\sigma(Wx^{(i)} + b) $.\n",
    "\n",
    "The goal is to calculate the gradient of $$ at a point $ \\mathbf{x} = (x_1, \\ldots, x_n) $, given by:\n",
    "\n",
    "$ \\nabla E = \\left(\\frac{\\partial}{\\partial w_1} E, \\ldots, \\frac{\\partial}{\\partial w_n} E, \\frac{\\partial}{\\partial b} E\\right) $\n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Derivative of Error with Respect to Weights</span>\n",
    "\n",
    "The derivative of the error with respect to weights is found by calculating $ \\frac{\\partial}{\\partial w_j} \\hat{y} $, where $ \\hat{y} = \\sigma(Wx + b) $:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j} \\hat{y} = \\hat{y}(1 - \\hat{y}) x_j $\n",
    "\n",
    "Now, let's proceed with the proof.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial w_j} \\hat{y} &= \\frac{\\partial \\sigma}{\\partial w_j} (Wx + b) \\\\\n",
    "&= \\sigma(Wx + b)(1 - \\sigma(Wx + b)) \\frac{\\partial}{\\partial w_j} (Wx + b) \\\\  \n",
    "&= \\hat{y}(1 - \\hat{y}) \\frac{\\partial}{\\partial w_j} (Wx + b)\\\\\n",
    "&= \\hat{y}(1 - \\hat{y}) \\frac{\\partial }{\\partial w_j} (w_1x_1  + \\ldots + w_jx_j + \\ldots + w_nx_n + b) \\\\  \n",
    "&= \\hat{y}(1 - \\hat{y}) x_j \\\\\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "### <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">Derivative of Error</span>\n",
    "\n",
    "The derivative of the error at a point $ \\mathbf{y}, \\mathbf{x} $, with respect to weight $ w_j $:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j} E = -(y - \\hat{y}) x_j $\n",
    "\n",
    "Now, we can go ahead and calculate the derivative of the error $E$ at a point $\\mathbf{y}, \\mathbf{x}$, with respect to the weight $w_j$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial w_j} E &= \\frac{\\partial}{\\partial w_j}[-y\\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})] \\\\\n",
    "&= -y \\frac{\\partial}{\\partial w_j} \\log(\\hat{y}) - (1 - y) \\frac{\\partial}{\\partial w_j} \\log(1 - \\hat{y}) \\\\\n",
    "&= -y \\frac{1}{\\hat{y}} \\frac{\\partial}{\\partial w_j}\\hat{y} - (1 - y) \\frac{1}{1 - \\hat{y}} \\frac{\\partial}{\\partial w_j}( 1- \\hat{y}) \\\\\n",
    "&= -y(1 - \\hat{y}) x_j + (1 - y)\\hat{y} x_j \\\\\n",
    "&= -(y - \\hat{y}) x_j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "A similar calculation shows that:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial b} = -(y - \\hat{y}) $\n",
    "\n",
    "In summary, the gradient is expressed as:\n",
    "\n",
    "$ \\nabla E = - (y - \\hat{y}) (x_1, \\ldots, x_n, 1) $\n",
    "\n",
    "This reveals that the gradient is a scalar multiple of the coordinates of the point, with the scalar being the difference between the label and the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz  \n",
    "What does the scalar we obtained above signify? (Check all that are true.)  \n",
    "- [] Closer the label to the prediction, larger the gradient.  \n",
    "- [x] Closer the label to the prediction, smaller the gradient.  \n",
    "- [x] Farther the label from the prediction, larger the gradient.  \n",
    "- [] Farther the label to the prediction, smaller the gradient.  \n",
    "\n",
    "So, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Step\n",
    "\n",
    "The gradient descent step involves updating the weights and bias based on the negative gradient of the error function at each point. The updates are performed as follows:\n",
    "\n",
    "For weights:\n",
    "$ w_i' \\leftarrow w_i - \\alpha \\cdot [-(y - \\hat{y}) \\cdot x_i] $\n",
    "\n",
    "This can be expressed as:\n",
    "$ w_i' \\leftarrow w_i + \\alpha \\cdot (y - \\hat{y}) \\cdot x_i $\n",
    "\n",
    "For the bias:\n",
    "$ b' \\leftarrow b + \\alpha \\cdot (y - \\hat{y}) $\n",
    "\n",
    "Here, $ \\alpha $ is the learning rate, and by considering the average of errors, we can represent it as $ \\frac{1}{m} \\cdot \\alpha $, but for simplicity, we denote it as $ \\alpha $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Decay\n",
    "\n",
    "If the learning rate is too large, the model takes huge steps, which might be fast at the beginning but could miss the local minimum and continue, leading to chaos. On the other hand, if the learning rate is too small, the model takes steady steps and has a better chance of reaching the local minimum but at the cost of more computations.\n",
    "\n",
    "**Decreasing Learning Rate General Guideline:**\n",
    "- If the gradient is steep, take longer steps.\n",
    "- If the terrain is plain, take smaller steps.  \n",
    "\n",
    "The global minimum is not always the best solution for a machine learning model, especially for deep neural networks. Sometimes, the global minimum may correspond to a very complex and overfitted model, while a local minimum may correspond to a simpler and more generalizable model. Therefore, the goal of learning rate decay is not necessarily to find the global minimum, but rather to find <span style=\"background-color: #FFFF99; color: #000000; font-weight: bold;\">a good balance between the optimization and generalization performance of the model </span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
